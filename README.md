# ğŸ§  Real-Time Video Frame Restoration via Student-Teacher Knowledge Distillation

## ğŸ“ Abstract

This project proposes a lightweight real-time video frame restoration framework based on **knowledge distillation**. A compact **student model**, capable of running at **30+ FPS** on low-end hardware (e.g., Jetson Nano, Raspberry Pi 4, or mid-range CPUs), is trained using supervision from a high-capacity **Restormer teacher model**.

The framework is designed for use in bandwidth-constrained or computationally limited environments such as **online classrooms**, **telemedicine**, **low-cost surveillance**, and **video conferencing**, where video frames are often degraded by **realistic motion blur, noise**, or mild compression artifacts.

By distilling knowledge from the Restormer model (CVPR 2022), the student model balances **efficiency and visual fidelity**, achieving strong quantitative (PSNR, SSIM) and subjective (MOS) results while maintaining real-time performance.

âœ… **The student model can also restore realistically blurred Full HD (1920Ã—1080) video frames â€” typical of video conferencing â€” at 30+ FPS**, making it ideal for edge device deployment in real-world streaming applications.

---

## ğŸ“¦ Prerequisites

### âœ… Clone Restormer Repository

To train or test with the teacher model, clone the official Restormer repository:

```bash
git clone https://github.com/swz30/Restormer.git
```

Ensure your project folder has the following structure:

```
ImageSharpening-KD/
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ student_output/
â”‚   â””â”€â”€ teacher_output/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.py
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”œâ”€â”€ extract_test.py
â”‚   â”œâ”€â”€ generate_patches.py
â”‚   â”œâ”€â”€ generate_test_input.py
â”‚   â”œâ”€â”€ degrade_video.py
â”‚   â””â”€â”€ analyze_dataset_quality.py
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ benchmark_model_fps.py
â”‚   â””â”€â”€ MOS_evaluation/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ student_model_trainer.py
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_student.py
â”‚   â”œâ”€â”€ test_student_video.py
â”‚   â””â”€â”€ test_restorer_teacher.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ Restormer/
    â””â”€â”€ Motion_Deblurring/
```

---

### ğŸ“¥ Download Restormer Checkpoint

You must download the pretrained Restormer Deblurring checkpoint:

ğŸ”— [Download `.pth` file](https://drive.google.com/file/d/1TDzcqvoNJS54yk7RSC-pco__HB4E32pz/view?usp=drive_link)

Then place it under:

```
Restormer/Motion_Deblurring/pretrained_models/
```

---

## ğŸ§ª Workflow

### 1. ğŸ“ Data Preparation

- **Download datasets**:
  ```bash
  python Scripts/download_data.py
  ```

- **Extract archives**:
  ```bash
  python Scripts/extract_data.py
  ```

- **Generate training patches**:
  ```bash
  python Scripts/generate_patches.py
  ```

- **Apply synthetic blur**:
  ```bash
  python Scripts/generate_test_input_blur.py
  ```

---

### 2. ğŸ§  Teacher Model (Restormer)

- The teacher model is based on the [Restormer architecture](https://github.com/swz30/Restormer) (CVPR 2022).
- It is pretrained and provides high-quality restoration performance.
- Used only for training the student model or as a comparison baseline.

#### ğŸ—‚ï¸ Output Folder Structure

```bash
outputs/
â”œâ”€â”€ student_output/
â”‚   â”œâ”€â”€ input/     # Blurred input frames
â”‚   â”œâ”€â”€ target/    # Ground truth (clean) frames
â”‚   â””â”€â”€ output/    # Restored frames from StudentNet
â”‚
â”œâ”€â”€ teacher_output/
â”‚   â”œâ”€â”€ input/     # Blurred input frames
â”‚   â”œâ”€â”€ target/    # Ground truth (clean) frames
â”‚   â””â”€â”€ output/    # Restored frames from Restormer
```

---

### 3. ğŸ‹ï¸ Train the Student Model

Train using knowledge distillation from the teacher:

```bash
python train.py
```

- Supports automatic checkpointing and resume.
- Learns via loss functions that include MSE, perceptual loss, and teacher supervision.

---

### 4. âœ… Testing & Evaluation

- **Download and extract test set**:  
  Manually download the ZIP from:  
  https://drive.google.com/file/d/1VPZTQGoDkdzldFk7PH1KH_vfIg1KwRq8/view?usp=drive_link  
  Place it in the project root directory and run:  
  ```bash
  python Scripts/extract_test.py
  ```
 
- **Test the student model**:
  ```bash
  python test/test_student.py
  ```

- **Test the teacher (Restormer) model**:
  ```bash
  python test/test_restormer_teacher.py
  ```

- **Evaluate SSIM, PSNR**:
  ```bash
  python evaluate/evaluate.py
  ```

- **Benchmark student model speed**:
  ```bash
  python evaluate/benchmark_model_fps.py
  ```

---

## 5. âœ… Video Testing & Degradation

- **Download the example videos:**  
  Manually download from:  
  [https://drive.google.com/drive/folders/1jWuOOYjfB6ELCkwQsJ9l4uX6LwxesY_6?usp=drive_link](https://drive.google.com/drive/folders/1jWuOOYjfB6ELCkwQsJ9l4uX6LwxesY_6?usp=drive_link)  
  Place them under:
```bash
  Output/student_output/input_student/
  Output/student_output/target_student/
  Output/student_output/output_student/
 ```
- Test the Model on Video
 ```bash
python test/test_student_video.py
 ```
- Output:
 ```bash
 Output/student_output/output_student/output_student_video_side_by_side.mp4**     
```
- **Simulate Realistic Conferencing Blur**
 ```bash
python Scripts/degrade_video.py
 ```

### 6.  ğŸ‘ï¸ Subjective Evaluation (MOS)

Subjective quality was assessed using the **Mean Opinion Score (MOS)** method.

The evaluation was performed using the `mos_evaluation.xlsx` file located in the `evaluation/MOS_evaluation/` directory. This spreadsheet includes:

- Ratings from **multiple human raters**
- Individual scores for **each test image**
- Automatically computed **average MOS score**

ğŸ“Š The overall average MOS achieved by the student model was:

```
â­ Average MOS: 4.13375 / 5.0
```

This indicates **high perceptual quality** of the restored outputs compared to degraded inputs, closely aligning with teacher-level performance as perceived by human observers.


---

## ğŸ“ File Descriptions

| File/Folder                   | Purpose                                                   |
|------------------------------|------------------------------------------------------------|
| `train.py`                   | Trains student model using teacher supervision             |
| `models/student_model.py`    | Lightweight model for real-time inference (30+ FPS)        |
| `models/teacher_model.py`    | Wrapper for pretrained Restormer model                     |
| `Scripts/generate_patches.py`        | Splits high-res images into training patches               |
| `evaluation/evaluate.py`                | Computes SSIM and PSNR metrics                             |
| `evaluation/benchmark_model_fps.py`     | Measures FPS of student model on target hardware           |
| `test/test_student.py`            | Runs student model on sample test data                     |
| `test/test_restormer_teacher.py`  | Runs teacher model and saves input, target, output images  |
| `Scripts/download_data.py`           | Downloads datasets (e.g., DIV2K, Unsplash)                 |
| `Scripts/extract_data.py`            | Unzips and organizes datasets                              |
| `evaluation/MOS_evaluation.xlsx/`            | Contains image-wise ratings from multiple raters and computed average MOS (4.13375)                       |
| `outputs/student_output/`   | Contains input (blurred), target (ground truth), and restored outputs from the student model |
| `output/teacher_output/`            | Contains input, target, and restored outputs from the teacher model (Restormer)      |

---

## âš™ï¸ Requirements

- Python 3.8+
- Install dependencies:

  ```bash
  pip install -r requirements.txt
  ```

- For Restormer-specific dependencies:

  [Restormer/INSTALL.md](https://github.com/swz30/Restormer/blob/main/INSTALL.md)

---

## âœ… Results Summary

| Metric        | Student Model       | Teacher Model (Restormer) |
|---------------|---------------------|----------------------------|
| MS-SSIM          | â‰¥ 0.90           | 0.94â€“0.96                  |
| PSNR          | 28â€“32 dB            | 32â€“35 dB                   |
| Inference FPS | **30â€“60+ (Real-Time)** | 3â€“6 FPS (Non-real-time)     |


- **Visual quality** is nearly indistinguishable from the teacher.
- **MOS results** show strong human preference over raw input.
- **Runs on edge devices** such as Raspberry Pi 4, Jetson Nano, or mid-range laptops.
- **Teacher output** is organized into `teacher_output/input/`, `target/`, and `output/` for visual and quantitative comparisons.

## ğŸ¥ Video Testing Results

| Metric     | Full HD Video (1920Ã—1080) |
|------------|---------------------------|
| SSIM       | 0.95                       |
| MS-SSIM    | 0.97                       |
| FPS        | 30+ (Real-time)            |

- **Tested on video sequences** using `python test_student_video.py`.
- Achieves high structural similarity on realistic conferencing degradation.

## ğŸ“ˆ Why MS-SSIM?

We use **Multi-Scale Structural Similarity Index (MS-SSIM)** as our primary evaluation metric.

âœ… Unlike standard SSIM (which measures similarity at a single scale), MS-SSIM evaluates image quality across multiple resolutions.  
This makes it more aligned with human visual perception, especially for assessing sharpness and structural fidelity.

âœ… MS-SSIM is now standard in image restoration research because it better captures how humans perceive improvements in texture and detail.

---

## ğŸ“ Performance Highlights

| Metric     | Single Image | 100-Image Average | Full HD Video |
|------------|--------------|-------------------|---------------|
| MS-SSIM    | 0.95         | 0.90              | 0.97          |
| SSIM       | 0.93         | 0.89              | 0.95          |
| FPS        | 30+          | Real-time         | Real-time     |


## ğŸ§© Deployment Note for Real-World Scenarios

This framework is optimized for **real-time restoration of lightly degraded video frames**, especially under conditions typical of:

- ğŸ¥ **Video conferencing**
- ğŸ‘©â€ğŸ« **Online classrooms**
- ğŸ¥ **Telemedicine**
- ğŸ¦ **Lightweight surveillance**
- ğŸ“± **Mobile and edge streaming**

In these scenarios, degradations are generally **minor**, including:

- Slight motion blur from camera shake
- Light Gaussian noise from compression or bandwidth constraints
- Mild JPEG artifacts


## ğŸš€ Future Scope
- Integrate temporal consistency to stabilize multi-frame sequences and reduce flicker.
- Apply quantization and pruning for ultra-constrained microcontroller deployment.
- Extend to handle severe motion blur and low-light scenarios.
- Explore multi-modal approaches with depth or inertial sensor data.
